


One thing I'd like you to look into is open source alternatives to Avatar that also have C/C++/Fortran (not just Python) bindings and can use 
off-line trained data.  Tensorflow is an obvious choice (though their build system requires Java of all things).  Others might include things like 
OpenNN, Torch or mlpack.  There's a bunch.  What I'd like you to do is:

1) Take a few days to inventory potential options.  They need:
  a) C, C++ or Fortran bindings (and not through weird callbacks to Python),  
  b) A suitable license for deployment in SNL codes (e.g. LGPL/MIT/BSD are OK, GPL is not). 
  c) To do training off-line, output the trained model to disk and then read it back in later.
  d) To provide machine learning algorithms, not just pieces from which you can roll your own.
 They ideally should:
  a) Be portable across platforms (e.g. they can use GPUs, but do not require them).
  b) Use reasonable TPLs for their linear algebra kernels (e.g. BLAS/LAPACK or CuBLAS) rather than rolling their own.
  c) Not rely too heavily on weird or hard to build TPLs.
2) Present your conclusions on which ones seem to be good options to our machine learning team.
3) Test them out on our MueLu data and see if they do a decent job (using Scikit Learn as a benchmark).
4) Implement interfaces to them in MueLu as alternatives to the Avatar interface you worked on last summer.




OpenNN
------
-C++
-License: LGPL
-Simple to produce analysis of results, including our old favorite confusion matrices
-Utilizes UML
-Can export mathematical expression of the model in plaintext, python, or an R script
-Provides pieces of NN algorithms, with which you specifiy and assemble NN 
-Is portable across platforms
-Does require Eigen library for linear algebra


Torch
-----
-Written in lua with C backend (disqualifying?)
-License: BSD
-Essentially a Tensor library like Numpy, but with strong GPU support
-Framework itself is low-level, but provides packages that make building NNs relatively simple


mlpack
------
-C++
-License: BSD
-Built on Armadillo linear algebra library (can be built w/o)
-Also requires ensmallen library
-Includes very similar ML algorithms to what we've used with Avatar (e.g. Random Forests w/ cross validation)
	-This could be nice, considering the success we've had with Avatar
-Simple to save and load trained models (.xml file)
-Also has support for SVMs, Linear Regression, other techniques (no NN support)


FANN
----
-Artificial NN library written in C
-License: LGPL
-Relatively simple to implement w/ 4 function calls
-Also simple to save and load previously created NNs
-http://fann.sourceforge.net/fann_en.pdf
-Wrapper for C++ could be useful
-file:///C:/Users/conds/Downloads/fann_doc_complete_1.0.pdf


Shogun
------
-C++
-License: BSD3
-API includes Random Forests, Nearest Neighbor, SVMs (other classification algorithms as well)
-Use save_model and load_model to load previously created model
-Lower-level
-Cannot run from command line like Avatar or mlpack

Dlib
----
-C++
-License: Boost
-API is broad ranging
-Limited number of classification algorithms
-Cannot run from command line



CNTK
----
-C++
-License: MIT
-Open source toolkit for distributed deep learning
-Seems more optimized for Python than C++


MXNet
-----
-C++
-License: Apache
-Slightly more involved than OpenNN and FANN
-Includes GPU support


Caffe
-----
-C++
-License: BSD
-Deep learning framework 
-Includes GPU support





Observations:
****************
-Thus far, it seems if we want to go the NN route, the algorithm itself will require quite a bit of tuning on our part
-FANN and OpenNN are very similar in practice; FANN has slightly higher level options; worth it to test out both 
-mlpack operates very similar to Avatar, so if we want to continue down that route, this is probably the way to go

-Next: test the compatible frameworks on our current dataset (which I do not currently have)
	
